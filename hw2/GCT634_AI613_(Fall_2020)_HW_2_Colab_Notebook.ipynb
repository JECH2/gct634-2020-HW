{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCT634/AI613 (Fall 2020) HW#2 Colab Notebook",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C34u1Fk_d855"
      },
      "source": [
        "# Homework #2: Music Genre Classification\n",
        "Music genre classification is an important task that can be used in many musical applications such as music search or recommender systems. Your mission is to build your own Convolutional Neural Network (CNN) model to classify audio files into different music genres. Specifically, the goals of this homework are as follows:\n",
        "\n",
        "* Experiencing the whole pipeline of deep learning based system: data preparation, feature extraction, model training and evaluation\n",
        "* Getting familiar with the CNN architectures for music classification tasks\n",
        "* Using Pytorch in practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS4CjH2Ocshx"
      },
      "source": [
        "# Getting Ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uXpEzF6HIry"
      },
      "source": [
        "## Installing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuoI2dH5JEML"
      },
      "source": [
        "!pip install musicnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5i1Y0_-cXv5"
      },
      "source": [
        "## Preparing The Dataset\n",
        "We use the [GTZAN](http://marsyas.info/downloads/datasets.html) dataset which has been the most widely used in the music genre classification task. \n",
        "The dataset contains 30-second audio files including 10 different genres including reggae, classical, country, jazz, metal, pop, disco, hiphop, rock and blues. \n",
        "For this homework, we are going to use a subset of GTZAN with only 8 genres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWuczGYl9sB3"
      },
      "source": [
        "# Download the dataset\n",
        "!gdown --id 1J1DM0QzuRgjzqVWosvPZ1k7MnBRG-IxS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEL8yjcO9zTs"
      },
      "source": [
        "# Uncompress the dataset\n",
        "!tar zxf gtzan.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMdtGjitcptx"
      },
      "source": [
        "## Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwAowYXSQSky"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "from glob import glob\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyC0UrXH-Uh6"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the Colab notebook:\n",
        "\n",
        "- Navigate to Edit (수정) → Notebook Settings (노트 설정)\n",
        "- select GPU from the Hardware Accelerator (하드웨어 가속기) drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with PyTorch and check versions of packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3RwC1lh-bCx"
      },
      "source": [
        "if not torch.cuda.is_available():\n",
        "  raise SystemError('GPU device not found!')\n",
        "print(f'Found GPU at: {torch.cuda.get_device_name()}')\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'Librosa version: {librosa.__version__}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IrP3qHvQuYi"
      },
      "source": [
        "If the cell above throws an error, then you should enable the GPU following the instruction above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEyf5th3zodi"
      },
      "source": [
        "# Training CNNs from Scratch\n",
        "\n",
        "The baseline code is provided so that you can easily start the homework and also compare with your own algorithm.\n",
        "The baseline model extracts mel-spectrogram and has a simple set of CNN model that includes convolutional layer, batch normalization, maxpooling and fully-connected layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLEXvCXUK6GR"
      },
      "source": [
        "## Extracting Mel-spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgjbChF0znXt"
      },
      "source": [
        "# Mel-spectrogram setup.\n",
        "SR = 16000\n",
        "FFT_HOP = 512\n",
        "FFT_SIZE = 1024\n",
        "NUM_MELS = 96"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAjStuDUI1Wb"
      },
      "source": [
        "genres = genres = ['classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae']\n",
        "genre_dict = {g: i for i, g in enumerate(genres)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19CimidPW0g1"
      },
      "source": [
        "def load_split(path):\n",
        "  with open(path) as f:\n",
        "    paths = [line.rstrip('\\n') for line in f]\n",
        "  return paths\n",
        "\n",
        "train = load_split('gtzan/split/train.txt')\n",
        "test = load_split('gtzan/split/test.txt')\n",
        "\n",
        "# Each entry of the lists look like this:\n",
        "len(train), len(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs179N8UXrrx"
      },
      "source": [
        "# Make directories to save mel-spectrograms.\n",
        "for genre in genres:\n",
        "  os.makedirs('gtzan/spec/' + genre, exist_ok=True)\n",
        "  \n",
        "for path_in in tqdm(train + test):\n",
        "  # The spectrograms will be saved under `gtzan/spec/` with an file extension of `.npy`\n",
        "  path_out = 'gtzan/spec/' + path_in.replace('.wav', '.npy')\n",
        "\n",
        "  # Skip if the spectrogram already exists\n",
        "  if os.path.isfile(path_out):\n",
        "    continue\n",
        "    \n",
        "  # Load the audio signal with the desired sampling rate (SR).\n",
        "  sig, _ = librosa.load(f'gtzan/wav/{path_in}', sr=SR, res_type='kaiser_fast')\n",
        "  # Compute power mel-spectrogram.\n",
        "  melspec = librosa.feature.melspectrogram(sig, sr=SR, n_fft=FFT_SIZE, hop_length=FFT_HOP, n_mels=NUM_MELS)\n",
        "  # Transform the power mel-spectrogram into the log compressed mel-spectrogram.\n",
        "  melspec = librosa.power_to_db(melspec)\n",
        "  # \"float64\" uses too much memory! \"float32\" has enough precision for spectrograms.\n",
        "  melspec = melspec.astype('float32')\n",
        "\n",
        "  # Save the spectrogram.\n",
        "  np.save(path_out, melspec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LykkQw0eLDWJ"
      },
      "source": [
        "## Defining a dataset of spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6YDccHNLdI1"
      },
      "source": [
        "# Data processing setup.\n",
        "BATCH_SIZE = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfeuWIyfiSsD"
      },
      "source": [
        "class SpecDataset(Dataset):\n",
        "  def __init__(self, paths, mean=0, std=1, time_dim_size=None):\n",
        "    self.paths = paths\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "    self.time_dim_size = time_dim_size\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    # Get i-th path.\n",
        "    path = self.paths[i]\n",
        "    # Get i-th spectrogram path.\n",
        "    path = 'gtzan/spec/' + path.replace('.wav', '.npy')\n",
        "\n",
        "    # Extract the genre from its path.\n",
        "    genre = path.split('/')[-2]\n",
        "    # Trun the genre into index number.\n",
        "    label = genre_dict[genre]\n",
        "\n",
        "    # Load the mel-spectrogram.\n",
        "    spec = np.load(path)\n",
        "    if self.time_dim_size is not None:\n",
        "      # Slice the temporal dimension with a fixed length so that they have\n",
        "      # the same temporal dimensionality in mini-batches.\n",
        "      spec = spec[:, :self.time_dim_size]\n",
        "    # Perform standard normalization using pre-computed mean and std.\n",
        "    spec = (spec - self.mean) / self.std\n",
        "\n",
        "    return spec, label\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7mZxLEgJ6ON"
      },
      "source": [
        "### Computing statistics of the training set\n",
        "The code below compute mean, standard deviation and the minimum temporal dimension size, and use them for preprocessing inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSNZ81-0nbWp"
      },
      "source": [
        "# Load all spectrograms.\n",
        "dataset_train = SpecDataset(train)\n",
        "specs = [s for s, _ in dataset_train]\n",
        "# Compute the minimum temporal dimension size.\n",
        "time_dims = [s.shape[1] for s in specs]\n",
        "min_time_dim_size = min(time_dims)\n",
        "# Stack the spectrograms\n",
        "specs = [s[:, :min_time_dim_size] for s in specs]\n",
        "specs = np.stack(specs)\n",
        "# Compute mean and standard deviation for standard normalization.\n",
        "mean = specs.mean()\n",
        "std = specs.std()\n",
        "\n",
        "min_time_dim_size, mean, std, "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6pspUSkKpYc"
      },
      "source": [
        "### Creating datasets and data loaders using the pre-computed statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6iSe9gni2M1"
      },
      "source": [
        "dataset_train = SpecDataset(train, mean, std, min_time_dim_size)\n",
        "dataset_test = SpecDataset(test, mean, std, min_time_dim_size)\n",
        "\n",
        "num_workers = os.cpu_count()\n",
        "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "loader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbODI4qpWYeo"
      },
      "source": [
        "## Training a baseline\n",
        "The table below shows the architecture of the baseline.\n",
        "\n",
        "| Layer          | Output Size | Details                 |\n",
        "|----------------|-------------|-------------------------|\n",
        "| conv           | 32 x 936    | kernel_size=7, stride=1 |\n",
        "| maxpool        | 32 x 133    | kernel_size=7, stride=7 |\n",
        "| conv           | 32 x 133    | kernel_size=7, stride=1 |\n",
        "| maxpool        | 32 x 19     | kernel_size=7, stride=7 |\n",
        "| conv           | 32 x 19     | kernel_size=7, stride=1 |\n",
        "| maxpool        | 32 x 2      | kernel_size=7, stride=7 |\n",
        "| global_avgpool | 32 x 1      | -                       |\n",
        "\n",
        "The class below is an implementation of it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2ZC-xNf2sgF"
      },
      "source": [
        "class Baseline(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Baseline, self).__init__()\n",
        "\n",
        "    self.conv0 = nn.Sequential(\n",
        "      nn.Conv1d(NUM_MELS, out_channels=32, kernel_size=7, stride=1, padding=3),\n",
        "      nn.BatchNorm1d(32),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool1d(kernel_size=7, stride=7)\n",
        "    )\n",
        "\n",
        "    self.conv1 = nn.Sequential(\n",
        "      nn.Conv1d(32, out_channels=32, kernel_size=7, stride=1, padding=3),\n",
        "      nn.BatchNorm1d(32),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool1d(kernel_size=7, stride=7)\n",
        "    )\n",
        "\n",
        "    self.conv2 = nn.Sequential(\n",
        "      nn.Conv1d(32, out_channels=32, kernel_size=7, stride=1, padding=3),\n",
        "      nn.BatchNorm1d(32),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool1d(kernel_size=7, stride=7)\n",
        "    )\n",
        "\n",
        "    # Aggregate features over temporal dimension.\n",
        "    self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    # Predict genres using the aggregated features.\n",
        "    self.linear = nn.Linear(32, len(genres))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv0(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.final_pool(x)\n",
        "    x = self.linear(x.squeeze(-1))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2fCKvL70Ydr"
      },
      "source": [
        "# Training setup.\n",
        "LR = 0.0006  # learning rate\n",
        "MOMENTUM = 0.9\n",
        "NUM_EPOCHS = 10\n",
        "weight_decay = 0.0  # L2 regularization weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChMVPF0XkmAu"
      },
      "source": [
        "model = Baseline()\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbzPhKsz01ZX"
      },
      "source": [
        "# Define a loss function, which is cross entropy here.\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# Setup an optimizer. Here, we use Stochastic gradient descent (SGD) with a nesterov mementum.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, nesterov=True, weight_decay=weight_decay)\n",
        "# Choose a device. We will use GPU if it's available, otherwise CPU.\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# Move variables to the desired device.\n",
        "model.to(device)\n",
        "criterion.to(device)\n",
        "\n",
        "print(f'Optimizer: {optimizer}')\n",
        "print(f'Device: {device}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABFnD3tN2CRw"
      },
      "source": [
        "# Util function for computing accuracy.\n",
        "def accuracy(source, target):\n",
        "  source = source.max(1)[1].long().cpu()\n",
        "  target = target.cpu()\n",
        "  correct = (source == target).sum().item()\n",
        "  return correct / float(source.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DBAh6pskuaY"
      },
      "source": [
        "# Set the status of the model as training.\n",
        "model.train()\n",
        "\n",
        "# Iterate over epochs.\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  pbar = tqdm(loader_train, desc=f'Epoch {epoch:02}')  # progress bar\n",
        "  for x, y in pbar:\n",
        "    # Move mini-batch to the desired device.\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Feed forward the model.\n",
        "    prediction = model(x)\n",
        "    # Compute the loss.\n",
        "    loss = criterion(prediction, y)\n",
        "    # Compute the accuracy.\n",
        "    acc = accuracy(prediction, y)\n",
        "\n",
        "    # Perform backward propagation to compute gradients.\n",
        "    loss.backward()\n",
        "    # Update the parameters.\n",
        "    optimizer.step()\n",
        "    # Reset the computed gradients.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Log training metrics.\n",
        "    batch_size = len(x)\n",
        "    epoch_loss += batch_size * loss.item()\n",
        "    epoch_acc += batch_size * acc\n",
        "    # Update the progress bar.\n",
        "    pbar.set_postfix({'loss': epoch_loss / len(dataset_train), \n",
        "                      'acc': epoch_acc / len(dataset_train)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsaLepTZk8RF"
      },
      "source": [
        "# Set the status of the model as evaluation.\n",
        "model.eval()\n",
        "\n",
        "# `torch.no_grad()` disables computing gradients. The gradients are still \n",
        "# computed even though you use `model.eval()`. You should use `torch.no_grad()` \n",
        "# if you don't want your memory is overflowed because of unnecesary gradients.\n",
        "with torch.no_grad():\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  pbar = tqdm(loader_test, desc=f'Test')  # progress bar\n",
        "  for x, y in pbar:\n",
        "    # Move mini-batch to the desired device.\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Feed forward the model.\n",
        "    prediction = model(x)\n",
        "    # Compute the loss.\n",
        "    loss = criterion(prediction, y)\n",
        "    # Compute the accuracy.\n",
        "    acc = accuracy(prediction, y)\n",
        "\n",
        "    # Log training metrics.\n",
        "    batch_size = len(x)\n",
        "    epoch_loss += batch_size * loss.item()\n",
        "    epoch_acc += batch_size * acc\n",
        "    # Update the progress bar.\n",
        "    pbar.set_postfix({'loss': epoch_loss / len(dataset_test), 'acc': epoch_acc / len(dataset_test)})\n",
        "\n",
        "# Compute the evaluation scores.\n",
        "test_loss = epoch_loss / len(dataset_test)\n",
        "test_acc = epoch_acc / len(dataset_test)\n",
        "\n",
        "print(f'test_loss={test_loss:.5f}, test_acc={test_acc * 100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Zwk73MGHmx"
      },
      "source": [
        "### [Question 1] Implement the given architecture.\n",
        "Implement a CNN with the architecture below, train, and report a test accuracy of the CNN.\n",
        "\n",
        "| Layer          | Output Size | Details                 |\n",
        "|----------------|-------------|-------------------------|\n",
        "| conv           | 16 x 936    | kernel_size=7, stride=1 |\n",
        "| maxpool        | 16 x 133    | kernel_size=7, stride=7 |\n",
        "| conv           | 32 x 133    | kernel_size=5, stride=1 |\n",
        "| maxpool        | 32 x 26     | kernel_size=5, stride=5 |\n",
        "| conv           | 64 x 26     | kernel_size=3, stride=1 |\n",
        "| maxpool        | 64 x 8      | kernel_size=3, stride=3 |\n",
        "| conv           | 128 x 8     | kernel_size=3, stride=1 |\n",
        "| maxpool        | 128 x 2     | kernel_size=3, stride=3 |\n",
        "| global_avgpool | 32 x 1      | -                       |\n",
        "\n",
        "Note: you should give appropriate paddings! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjP6-Q6Lhqwi"
      },
      "source": [
        "# TODO: Question 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIRc_WML8MnD"
      },
      "source": [
        "# Exploiting Prior Knowledge using Pre-trained Models\n",
        "\n",
        "\n",
        "Someone who knows how to play acoustic guitars might be better at playing electric guitars than who never played a guitar.\n",
        "Here, we will use pre-trained models from [`musicnn`](https://github.com/jordipons/musicnn) (pronounced as \"musician\"), which includes CNNs already trained on a large amount of songs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq4wDvyMTTOk"
      },
      "source": [
        "You can predict some tags with the pre-trained model like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQkmEKSXTfW5"
      },
      "source": [
        "from musicnn.tagger import top_tags\n",
        "\n",
        "_ = top_tags('gtzan/wav/' + train[0], model='MTT_musicnn', topN=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTo0E7zdThUm"
      },
      "source": [
        "However, the 10 tags are not what we want as outputs! Let's extract embedding (or features) using the pre-trained model, and train 2-layer MLP using the embeddings as inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz0e6KvNTwMh"
      },
      "source": [
        "## Extracting embeddings using the pre-trained model\n",
        "\n",
        "Side note: this will take about 23 min."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L-FJ-e5ISbg"
      },
      "source": [
        "from musicnn.extractor import extractor\n",
        "\n",
        "# Make directories to save embeddings.\n",
        "for genre in genres:\n",
        "  os.makedirs('gtzan/embed/' + genre, exist_ok=True)\n",
        "\n",
        "for path_in in tqdm(train + test):\n",
        "  # The embeddings will be saved under `gtzan/embed/` with an file extension of `.npy`\n",
        "  path_out = 'gtzan/embed/' + path_in.replace('.wav', '.npy')\n",
        "  # Skip if the embedding already exists.\n",
        "  if os.path.isfile(path_out):\n",
        "    continue\n",
        "  \n",
        "  # Extract the embedding using the pre-trained model.\n",
        "  _, _, embeds = extractor(f'gtzan/wav/{path_in}', model='MTT_musicnn', extract_features=True)\n",
        "  # Average the embeddings over temporal dimension.\n",
        "  embed = embeds['max_pool'].mean(axis=0)\n",
        "\n",
        "  # Save the embedding.\n",
        "  np.save(path_out, embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmgDstz0L_PE"
      },
      "source": [
        "class EmbedDataset(Dataset):\n",
        "  def __init__(self, paths):\n",
        "    self.paths = paths\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    # Get i-th path.\n",
        "    path = self.paths[i]\n",
        "    # Get i-th embeddding path.\n",
        "    path = 'gtzan/embed/' + path.replace('.wav', '.npy')\n",
        "\n",
        "    # Extract the genre from its path.\n",
        "    genre = path.split('/')[-2]\n",
        "    # Trun the genre into index number.\n",
        "    label = genre_dict[genre]\n",
        "\n",
        "    # Load the mel-spectrogram.\n",
        "    embed = np.load(path)\n",
        "\n",
        "    return embed, label\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lywWIQj5UgYe"
      },
      "source": [
        "dataset_train = EmbedDataset(train)\n",
        "dataset_test = EmbedDataset(test)\n",
        "\n",
        "num_workers = os.cpu_count()\n",
        "loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "loader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXo3AIaNPvCJ"
      },
      "source": [
        "embed_size = dataset_train[0][0].shape[0]\n",
        "embed_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY0snpd1cREq"
      },
      "source": [
        "### [Question 2] Implement, train and evaluate 2-layer MLP using the extracted embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNhEh_76Ps25"
      },
      "source": [
        "# TODO: Question 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33p0jbUcjZKF"
      },
      "source": [
        "# Improving Algorithms [[Leader Board]](https://docs.google.com/spreadsheets/d/1bzkMFeXABTae7kDJG6QCU_qnP1ppJDoNQLgGz3ksJu0/edit?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef-Qjx1rdYEY"
      },
      "source": [
        "### [Question 3] Improve the performenace.\n",
        "Now it is your turn. You should improve the baseline code with your own algorithm. There are many ways to improve it. The followings are possible ideas: \n",
        "\n",
        "* The first thing to do is to segment audio clips and generate more data. The baseline code utilizes the whole mel-spectrogram as an input to the network (e.g. 96x936 dimensions). Try to make the network input between 3-5 seconds segment and average the predictions of the segmentations for an audio clip.\n",
        "\n",
        "* You can try training a model using both mel-spectrograms and features extracted using the pre-trained models. The baseline code is using a pre-trained model trained on 19k songs, but `musicnn` also has models trained on 200k songs! Try using the model giving `model='MSD_musicnn'` option on feature extraction.\n",
        "\n",
        "* You can try 1D CNN or 2D CNN models and choose different model parameters:\n",
        "    * Filter size\n",
        "    * Pooling size\n",
        "    * Stride size \n",
        "    * Number of filters\n",
        "    * Model depth\n",
        "    * Regularization: L2/L1 and Dropout\n",
        "\n",
        "* You should try different hyperparameters to train the model and optimizers:\n",
        "    * Learning rate\n",
        "    * Model depth\n",
        "    * Optimizers: SGD (with Nesterov momentum), Adam, RMSProp, ...\n",
        "\n",
        "* You can try different parameters (e.g. hop and window size) to extract mel-spectrogram or different features as input to the network (e.g. MFCC, chroma features ...). \n",
        "\n",
        "* You can also use ResNet or other CNNs with skip connections. \n",
        "\n",
        "* Furthermore, you can augment data using digital audio effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqTgCmU5h32G"
      },
      "source": [
        "# TODO: Question 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRfOAefCh57y"
      },
      "source": [
        "\n",
        "# Deliverables\n",
        "You should submit your Python code (`.ipynb` or `.py` files) and homework report (.pdf file) to KLMS. The report should include:\n",
        "* Algorithm Description\n",
        "* Experiments and Results\n",
        "* Discussion\n",
        "\n",
        "# Note\n",
        "The code is written using PyTorch but you can use TensorFlow if you want for question 3.\n",
        "\n",
        "# Credit\n",
        "Thie homework was implemented by Jongpil Lee, Soonbeom Choi and Taejun Kim in the KAIST Music and Audio Computing Lab.\n"
      ]
    }
  ]
}